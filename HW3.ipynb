{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №3\n",
    "## Задание №1\n",
    "Скачайте текст [\"Литературных анекдотов\"](https://github.com/ancatmara/python-for-dh/blob/master/Classes/9-10/literary_anecdotes.txt) . Напишите функцию, которая будет читать файл, лемматизировать текст с помощью pymystem3 и записывать результат в новый файл. \n",
    "У функции должно бы два аргумента: путь к исходному файлу и путь к файлу с лемматизированным текстом. Вызов функции тоже должен быть прописан в решении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem   #импортируем нужные модули\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "os.chdir('C:\\\\Users\\\\User\\\\Desktop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmitize(input_txt, output_txt):          #создаем функцию леммитизации\n",
    "    with open(input_txt, 'r', encoding='utf-8') as f:     #открываем  текст, делаем список лемм\n",
    "        text = f.read()\n",
    "        m = Mystem()\n",
    "        ftext = re.sub('\\W',' ',text)                #убираем пунктуацию \n",
    "        lemmas = m.lemmatize(ftext)\n",
    "    g = open(output_txt, 'tw', encoding='utf-8')     #создаем текстовый файл и записываем туда леммы  \n",
    "    g.write(''.join(lemmas))\n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmitize('literary_anecdotes.txt','text.txt')    #производим леммитизацию с нашими директориями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание №2\n",
    "Очистите лемматизированный текст от стоп-слов и посчитайте ipm для оставшихся. Выведите 20 самых частотных по этому параметру слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('пушкин', 35329.341317365266), ('толстой', 19760.47904191617), ('гоголь', 18562.874251497007), ('однажды', 17365.269461077845), ('лев', 14970.059880239522), ('любить', 11976.047904191617), ('достоевский', 11976.047904191617), ('тургенев', 9580.838323353293), ('царствие', 8982.035928143712), ('ребенок', 8982.035928143712), ('небесный', 8982.035928143712), ('окно', 7185.62874251497), ('лермонтов', 7185.62874251497), ('тверской', 7185.62874251497), ('бульвар', 7185.62874251497), ('приходить', 7185.62874251497), ('михайлович', 6586.8263473053885), ('федор', 6586.8263473053885), ('идти', 5988.023952095808), ('человек', 5988.023952095808)]\n"
     ]
    }
   ],
   "source": [
    "with open('rus_stopwords.txt', 'r', encoding='utf-8') as file:      #открываем файл со стоп-словами, создаем список таких слов\n",
    "    stops = file.read()\n",
    "    words = stops.split()\n",
    "    with open('text.txt', 'r', encoding='utf-8') as file1:      #открываем список лемм\n",
    "        text = file1.read()\n",
    "        list1 = text.split()\n",
    "        for word in words:                       #для каждого стоп-слова проверяем в списке лемм его наличие, при нахождении удаляем\n",
    "            for text_word in list1:\n",
    "                if (word==text_word):\n",
    "                    list1.remove(text_word)\n",
    "set1 = set(list1)\n",
    "count = len(list1)\n",
    "d = dict()\n",
    "for set_word in set1:           #считаем частотность каждого слова, сортируем словарь и выводим первые 20\n",
    "    d[set_word]=list1.count(set_word)/ count * 1000000\n",
    "sorted_d = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_d[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание №3\n",
    "Сделайте полный морфологический разбор исходного текста. Напишите регулярное выражение, которое будет извлекать из тега только часть речи. Пройдитесь циклом по списку с разборами, который выдал pymystem3, извлекая из каждого разбора форму слова и его часть речи и записывая их в новый словарь (форма -- ключ, часть речи -- значение). Посчитайте абсолютную частоту для всех частей речи, а затем относительнную частоту (абсолютная частота / длина текста в словах)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('literary_anecdotes.txt', 'r', encoding='utf-8') as j:    #записываем исходный текст в переменную text3\n",
    "        text3 = j.read()                                           \n",
    "m = Mystem()\n",
    "latte = m.analyze(text3)                                            #проводим морфологический анализ\n",
    "with open('razbor.json', 'w', encoding='utf-8') as r:\n",
    "    json.dump(latte, r, ensure_ascii=False)\n",
    "with open('razbor.json', 'r', encoding='utf-8') as q:\n",
    "    morph = json.loads(q.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pent = re.compile('[A-Z]+')   #создаем регулярное выражение для поиска части слова\n",
    "v = dict()                    #создаем словарь  \n",
    "chasti = ''\n",
    "for i in morph:                 #в цикле достаем из морфологических разборов части речи и формы слов \n",
    "    for key,value in v:\n",
    "        try:\n",
    "            key = re.search(\"\\\"text\\\": \\\".*\\\"\", i) #с помощью регулярного выражения достаем нужную часть с формой слова\n",
    "            key = key[9:-1]                        #достаем само слово\n",
    "            value = pent.search(i)                 #ищем часть слова\n",
    "            chasti += ' ' + value                  #создаем строку с частями слова для последующего анализа\n",
    "        except (KeyError, IndexError) as e:        #исключаем ошибки\n",
    "            pass \n",
    "chasti2 = chasti.split()\n",
    "set2 = set(chasti2)\n",
    "count2 = len(text3.split())\n",
    "abschastota = dict()\n",
    "otnchastota = dict()\n",
    "for set2_word in set2: #считаем абсолютную и относительную частотность каждой части речи\n",
    "    abschastota[set2_word] = chasti2.count(set2_word)\n",
    "    otnchastota[set2_word] = chasti2.count(set2_word)/ count * 1000000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
